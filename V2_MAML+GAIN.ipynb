{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V2-MAML+GAIN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkP5aBdfFkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdcd558-9bc8-45ee-a2cc-7075360d5205"
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "if not os.path.isdir('./omniglot_resized'):\n",
        "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
        "                                        dest_path='./omniglot_resized.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "assert os.path.isdir('./omniglot_resized')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI into ./omniglot_resized.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtiYUiwI-1K"
      },
      "source": [
        "\"\"\" Utility functions. \"\"\"\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "## Loss utilities\n",
        "def cross_entropy_loss(pred, label, k_shot):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "  return tf.keras.losses.MSE(y_true, y_pred)\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
        "    "
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXZS_JULriBh"
      },
      "source": [
        "\"\"\"Data loading scripts\"\"\"\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "import imageio\n",
        "\n",
        "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "    paths: A list of character folders\n",
        "    labels: List or numpy array of same length as paths\n",
        "    n_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "    List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if n_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, n_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "           for i, path in zip(labels, paths)\n",
        "           for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "    filename: Image filename\n",
        "    dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "    1 channel image\n",
        "  \"\"\"\n",
        "  image = imageio.imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      num_classes: Number of classes for classification (K-way)\n",
        "      num_samples_per_class: num samples to generate per class in one batch\n",
        "      num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
        "      num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
        "      batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "    self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
        "    self.num_meta_test_classes = num_meta_test_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    # self.dim_output = self.num_classes\n",
        "    self.dim_output = np.prod(self.img_size)\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "               for family in os.listdir(data_folder)\n",
        "               if os.path.isdir(os.path.join(data_folder, family))\n",
        "               for character in os.listdir(os.path.join(data_folder, family))\n",
        "               if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(123)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "      num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "      batch_type: meta_train/meta_val/meta_test\n",
        "      shuffle: randomly shuffle classes or not\n",
        "      swap: swap number of classes (N) and number of samples per class (K) or not\n",
        "    Returns:\n",
        "      A a tuple of (1) Image batch and (2) Label batch where\n",
        "      image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
        "      where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"meta_train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    elif batch_type == \"meta_val\":\n",
        "      folders = self.metaval_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "      num_classes = self.num_meta_test_classes\n",
        "      num_samples_per_class = self.num_meta_test_samples_per_class\n",
        "    all_image_batches, all_label_batches = [], []\n",
        "    for i in range(batch_size):\n",
        "      sampled_character_folders = random.sample(\n",
        "        folders, num_classes)\n",
        "      labels_and_images = get_images(sampled_character_folders, range(\n",
        "        num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
        "      labels = [li[0] for li in labels_and_images]\n",
        "      images = [image_file_to_array(\n",
        "        li[1], self.dim_input) for li in labels_and_images]\n",
        "      images = np.stack(images)\n",
        "      labels = np.array(labels).astype(np.int32)\n",
        "      labels = np.reshape(\n",
        "        labels, (num_classes, num_samples_per_class))\n",
        "      labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
        "      images = np.reshape(\n",
        "        images, (num_classes, num_samples_per_class, -1))\n",
        "\n",
        "      batch = np.concatenate([labels, images], 2)\n",
        "      if shuffle:\n",
        "        for p in range(num_samples_per_class):\n",
        "          np.random.shuffle(batch[:, p])\n",
        "\n",
        "      # labels = batch[:, :, :num_classes]\n",
        "      images = batch[:, :, num_classes:]\n",
        "\n",
        "      if swap:\n",
        "        # labels = np.swapaxes(labels, 0, 1)\n",
        "        images = np.swapaxes(images, 0, 1)\n",
        "\n",
        "      all_image_batches.append(images)\n",
        "      # all_label_batches.append(labels)\n",
        "    all_image_batches = np.stack(all_image_batches)\n",
        "    # all_label_batches = np.stack(all_label_batches)\n",
        "    # return all_image_batches, all_label_batches\n",
        "    return all_image_batches, all_image_batches"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_dTnU8JwWWc"
      },
      "source": [
        "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
        "seed = 123\n",
        "\n",
        "class GeneratorNetwork(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
        "    super(GeneratorNetwork, self).__init__()\n",
        "    self.channels = channels\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.dim_output = dim_output\n",
        "    self.img_size = img_size\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    dtype = tf.float32\n",
        "    weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
        "\n",
        "    weights['G_W1'] = tf.Variable(weight_initializer(shape=[784 * self.channels, self.dim_hidden]), name='G_W1', dtype=dtype)  \n",
        "    weights['G_b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='G_b1')\n",
        "    \n",
        "    weights['G_W2'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_hidden]), name='G_W2', dtype=dtype)  \n",
        "    weights['G_b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='G_b2')\n",
        "\n",
        "    weights['G_W3'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_output]), name='G_W3', dtype=dtype)  \n",
        "    weights['G_b3'] = tf.Variable(tf.zeros([self.dim_output]), name='G_b3')\n",
        "\n",
        "    self.net_weights = weights\n",
        "\n",
        "  def call(self, inp, weights):\n",
        "    # Concatenate Mask and Data\n",
        "    # x, m = inp\n",
        "    # inputs = tf.concat(values = inp, axis = 1) \n",
        "\n",
        "    G_h1 = tf.nn.relu(tf.matmul(inp, weights['G_W1']) + weights['G_b1'])\n",
        "    G_h2 = tf.nn.relu(tf.matmul(G_h1, weights['G_W2']) + weights['G_b2'])   \n",
        "    # MinMax normalized output\n",
        "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, weights['G_W3']) + weights['G_b3']) \n",
        "    return G_prob"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKoUSF0ZicKQ"
      },
      "source": [
        "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
        "seed = 123\n",
        "\n",
        "class DiscriminatorNetwork(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
        "    super(DiscriminatorNetwork, self).__init__()\n",
        "    self.channels = channels\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.dim_output = dim_output\n",
        "    self.img_size = img_size\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    dtype = tf.float32\n",
        "    weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
        "\n",
        "    weights['D_W1'] = tf.Variable(weight_initializer(shape=[784, self.dim_hidden]), name='D_W1', dtype=dtype) # Data + Hint as inputs\n",
        "    weights['D_b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='D_b1')\n",
        "  \n",
        "    weights['D_W2'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_hidden]), name='D_W2', dtype=dtype)\n",
        "    weights['D_b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='D_b2')\n",
        "  \n",
        "    weights['D_W3'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_input]), name='D_W3', dtype=dtype)\n",
        "    weights['D_b3'] = tf.Variable(tf.zeros([self.dim_output]), name='D_b3')\n",
        "\n",
        "    self.net_weights = weights\n",
        "\n",
        "  def call(self, inp, weights):\n",
        "    # inputs = tf.concat(values = [x, h], axis = 1) \n",
        "    D_h1 = tf.nn.relu(tf.matmul(inp, weights['D_W1']) + weights['D_b1'])  \n",
        "    D_h2 = tf.nn.relu(tf.matmul(D_h1, weights['D_W2']) + weights['D_b2'])\n",
        "    D_logit = tf.matmul(D_h2, weights['D_W3']) + weights['D_b3']\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "    return D_prob"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxriXFvwsGfp"
      },
      "source": [
        "\"\"\"MAML model code\"\"\"\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "  def __init__(self, dim_input=1, dim_output=1,\n",
        "               num_inner_updates=1,\n",
        "               inner_update_lr=0.4, dim_hidden=300, k_shot=5, learn_inner_update_lr=False, network_type=GeneratorNetwork):\n",
        "    super(MAML, self).__init__()\n",
        "    self.dim_input = dim_input\n",
        "    self.dim_output = dim_output\n",
        "    self.inner_update_lr = inner_update_lr\n",
        "    self.loss_func = mse_loss\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.channels = 1\n",
        "    self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
        "\n",
        "    # outputs_ts[i] and losses_ts_post[i] are the output and loss after i+1 inner gradient updates\n",
        "    losses_tr_pre, outputs_tr, losses_ts_post, outputs_ts = [], [], [], []\n",
        "    # accuracies_tr_pre, accuracies_ts = [], []\n",
        "\n",
        "    # for each loop in the inner training loop\n",
        "    outputs_ts = [[]]*num_inner_updates\n",
        "    losses_ts_post = [[]]*num_inner_updates\n",
        "    # accuracies_ts = [[]]*num_inner_updates\n",
        "\n",
        "    # Define the weights - these should NOT be directly modified by the\n",
        "    # inner training loop\n",
        "    tf.random.set_seed(seed)\n",
        "    self.gen_network_layers = network_type(self.channels, self.dim_hidden, self.dim_output, self.img_size)\n",
        "\n",
        "    self.learn_inner_update_lr = learn_inner_update_lr\n",
        "    if self.learn_inner_update_lr:\n",
        "      self.inner_update_lr_dict = {}\n",
        "      for key in self.gen_network_layers.net_weights.keys():\n",
        "        self.inner_update_lr_dict[key] = [tf.Variable(self.inner_update_lr, name='inner_update_lr_%s_%d' % (key, j)) for j in range(num_inner_updates)]\n",
        "  \n",
        "\n",
        "  def call(self, inp, meta_batch_size=25, num_inner_updates=1, other_network):\n",
        "    def task_inner_loop(inp, reuse=True,\n",
        "                      meta_batch_size=25, num_inner_updates=1):\n",
        "      \"\"\"\n",
        "        Perform gradient descent for one task in the meta-batch (i.e. inner-loop).\n",
        "        Args:\n",
        "          inp: a tuple (input_tr, input_ts, label_tr, label_ts), where input_tr and label_tr are the inputs and\n",
        "            labels used for calculating inner loop gradients and input_ts and label_ts are the inputs and\n",
        "            labels used for evaluating the model after inner updates.\n",
        "            Should be shapes:\n",
        "              input_tr: [N*K, 784]\n",
        "              input_ts: [N*K, 784]\n",
        "              label_tr: [N*K, N]\n",
        "              label_ts: [N*K, N]\n",
        "        Returns:\n",
        "          task_output: a list of outputs, losses and accuracies at each inner update\n",
        "      \"\"\"\n",
        "      # the inner and outer loop data\n",
        "      input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "      # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
        "      weights = self.gen_network_layers.net_weights\n",
        "\n",
        "      # the predicted outputs, loss values, and accuracy for the pre-update model (with the initial weights)\n",
        "      # evaluated on the inner loop training data\n",
        "      task_output_tr_pre, task_loss_tr_pre = None, None\n",
        "      # task_output_tr_pre, task_loss_tr_pre, task_accuracy_tr_pre = None, None, None\n",
        "\n",
        "      # lists to keep track of outputs, losses, and accuracies of test data for each inner_update\n",
        "      # where task_outputs_ts[i], task_losses_ts[i], task_accuracies_ts[i] are the output, loss, and accuracy\n",
        "      # after i+1 inner gradient updates\n",
        "      task_outputs_ts, task_losses_ts = [], []\n",
        "      # task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
        "  \n",
        "      # perform num_inner_updates to get modified weights\n",
        "      # modified weights should be used to evaluate performance\n",
        "      # Note that at each inner update, always use input_tr and label_tr for calculating gradients\n",
        "      # and use input_ts and labels for evaluating performance\n",
        "\n",
        "      # https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
        "      # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "      with tf.GradientTape() as inner_tape:\n",
        "        # TRAIN 0\n",
        "        task_output_tr_pre = self.gen_network_layers(input_tr, weights)\n",
        "        task_loss_tr_pre = self.loss_func(task_output_tr_pre, label_tr)\n",
        "        grads = inner_tape.gradient(task_loss_tr_pre, list(weights.values()))\n",
        "        gradients = dict(zip(weights.keys(), grads))\n",
        "        updated_weights = dict(zip(weights.keys(), [weights[key] - self.learn_inner_update_lr*gradients[key] for key in weights.keys()]))\n",
        "\n",
        "        # EVAL WITH TEST 0\n",
        "        task_outputs_ts.append(self.gen_network_layers(input_ts, updated_weights))\n",
        "        task_losses_ts.append(self.loss_func(task_outputs_ts[0], label_ts))\n",
        "        \n",
        "        for i in range(num_inner_updates-1):\n",
        "          # TRAIN i\n",
        "          task_output_tr = self.gen_network_layers(input_tr, updated_weights)\n",
        "          task_loss_tr = self.loss_func(task_output_tr, label_tr)\n",
        "          grads = inner_tape.gradient(task_loss_tr, list(updated_weights.values()))\n",
        "          gradients = dict(zip(updated_weights.keys(), grads))\n",
        "          updated_weights = dict(zip(updated_weights.keys(), [updated_weights[key] - self.learn_inner_update_lr*gradients[key] for key in updated_weights.keys()]))\n",
        "        \n",
        "          # EVAL WITH TEST i\n",
        "          task_outputs_ts.append(self.gen_network_layers(input_ts, updated_weights))\n",
        "          task_losses_ts.append(self.loss_func(task_outputs_ts[i], label_ts))\n",
        "          \n",
        "      #############################\n",
        "\n",
        "      # Compute accuracies from output predictions\n",
        "      # task_accuracy_tr_pre = accuracy(tf.argmax(input=label_tr, axis=1), tf.argmax(input=tf.nn.softmax(task_output_tr_pre), axis=1))\n",
        "\n",
        "      # for j in range(num_inner_updates):\n",
        "      #   task_accuracies_ts.append(accuracy(tf.argmax(input=label_ts, axis=1), tf.argmax(input=tf.nn.softmax(task_outputs_ts[j]), axis=1)))\n",
        "\n",
        "      # task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
        "      task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts]\n",
        "\n",
        "      return task_output\n",
        "\n",
        "    input_tr, input_ts, label_tr, label_ts = inp\n",
        "    # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
        "    unused = task_inner_loop((input_tr[0], input_ts[0], label_tr[0], label_ts[0]),\n",
        "                          False,\n",
        "                          meta_batch_size,\n",
        "                          num_inner_updates)\n",
        "    out_dtype = [tf.float32, [tf.float32]*num_inner_updates, tf.float32, [tf.float32]*num_inner_updates]\n",
        "    # out_dtype.extend([tf.float32, [tf.float32]*num_inner_updates])\n",
        "    task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "    result = tf.map_fn(task_inner_loop_partial,\n",
        "                    elems=(input_tr, input_ts, label_tr, label_ts),\n",
        "                    dtype=out_dtype,\n",
        "                    parallel_iterations=meta_batch_size)\n",
        "    return result\n",
        "   "
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1pz_ousUsz"
      },
      "source": [
        "\"\"\"Model training code\"\"\"\n",
        "\"\"\"\n",
        "Usage Instructions:\n",
        "  5-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=25 --k_shot=1 --inner_update_lr=0.4 --num_inner_updates=1 --logdir=logs/omniglot5way/\n",
        "  20-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=16 --k_shot=1 --n_way=20 --inner_update_lr=0.1 --num_inner_updates=5 --logdir=logs/omniglot20way/\n",
        "  To run evaluation, use the '--meta_train=False' flag and the '--meta_test_set=True' flag to use the meta-test set.\n",
        "\"\"\"\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "def outer_train_step(inp, model, optim, meta_batch_size=25, num_inner_updates=1):\n",
        "  with tf.GradientTape(persistent=False) as outer_tape:\n",
        "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    # outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "    outputs_tr, outputs_ts, losses_tr_pre, losses_ts = result\n",
        "\n",
        "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  # total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  # total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts#, total_accuracy_tr_pre, total_accuracies_ts\n",
        "\n",
        "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
        "  result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "  # outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "  outputs_tr, outputs_ts, losses_tr_pre, losses_ts = result\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  # total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  # total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts#, total_accuracy_tr_pre, total_accuracies_ts  \n",
        "\n",
        "\n",
        "def meta_train_fn(model, exp_string, data_generator,\n",
        "               n_way=5, meta_train_iterations=15000, meta_batch_size=25,\n",
        "               log=True, logdir='/tmp/data', k_shot=1, num_inner_updates=1, meta_lr=0.001):\n",
        "  SUMMARY_INTERVAL = 10\n",
        "  SAVE_INTERVAL = 100\n",
        "  PRINT_INTERVAL = 10  \n",
        "  TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
        "\n",
        "  # pre_accuracies, post_accuracies = [], []\n",
        "  pre_losses, post_losses = [], []\n",
        "\n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
        "\n",
        "  for itr in range(meta_train_iterations):\n",
        "    # sample a batch of training data and partition into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    batch = data_generator.sample_batch(\"meta_train\", meta_batch_size, shuffle=False, swap=False)\n",
        "    #batch[0].shape(images) = (meta_batch_size, num_classes, k_shot*2, 784)\n",
        "    #batch[1].shape(labels) = (meta_batch_size, num_classes, k_shot*2, num_classes)\n",
        "    input_tr, input_ts = tf.split(batch[0], num_or_size_splits=2, axis=2)\n",
        "    input_tr = tf.reshape(input_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    input_ts = tf.reshape(input_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    label_tr, label_ts = tf.split(batch[1], num_or_size_splits=2, axis=2)\n",
        "    label_tr = tf.reshape(label_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    label_ts = tf.reshape(label_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    \n",
        "    result = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    if itr % SUMMARY_INTERVAL == 0:\n",
        "      # pre_accuracies.append(result[-2])\n",
        "      # post_accuracies.append(result[-1][-1])\n",
        "      pre_losses.append(result[2])\n",
        "      post_losses.append(result[3][-1])\n",
        "\n",
        "    if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
        "      # print_str = 'Iteration %d: pre-inner-loop train accuracy: %.5f, post-inner-loop test accuracy: %.5f' % (itr, np.mean(pre_accuracies), np.mean(post_accuracies))\n",
        "      # print(print_str)\n",
        "      # pre_accuracies, post_accuracies = [], []\n",
        "      print_str = 'Iteration %d: pre-inner-loop train loss: %.5f, post-inner-loop test loss: %.5f' % (itr, np.mean(pre_losses), np.mean(post_losses))\n",
        "      print(print_str)\n",
        "      pre_losses, post_losses = [], []\n",
        "\n",
        "    if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0:\n",
        "      # sample a batch of validation data and partition it into\n",
        "      # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "      # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "      batch = data_generator.sample_batch(\"meta_val\", meta_batch_size, shuffle=False, swap=False)\n",
        "      #batch[0].shape(images) = (meta_batch_size, num_classes, k_shot*2, 784)\n",
        "      #batch[1].shape(labels) = (meta_batch_size, num_classes, k_shot*2, num_classes)\n",
        "      input_tr, input_ts = tf.split(batch[0], num_or_size_splits=2, axis=2)\n",
        "      input_tr = tf.reshape(input_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "      input_ts = tf.reshape(input_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "      label_tr, label_ts = tf.split(batch[1], num_or_size_splits=2, axis=2)\n",
        "      label_tr = tf.reshape(label_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "      label_ts = tf.reshape(label_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "\n",
        "      inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "      result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "      print('Meta-validation pre-inner-loop train loss: %.5f, meta-validation post-inner-loop test loss: %.5f' % (result[2], result[3][-1]))\n",
        "      # print('Meta-validation pre-inner-loop train accuracy: %.5f, meta-validation post-inner-loop test accuracy: %.5f' % (result[-2], result[-1][-1]))\n",
        "\n",
        "  model_file = logdir + '/' + exp_string +  '/model' + str(itr)\n",
        "  print(\"Saving to \", model_file)\n",
        "  model.save_weights(model_file)\n",
        "\n",
        "# calculated for omniglot\n",
        "NUM_META_TEST_POINTS = 600\n",
        "\n",
        "def meta_test_fn(model, data_generator, n_way=5, meta_batch_size=25, k_shot=1,\n",
        "              num_inner_updates=1):\n",
        "  \n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "\n",
        "  # meta_test_accuracies = []\n",
        "  meta_test_losses = []\n",
        "\n",
        "  for _ in range(NUM_META_TEST_POINTS):\n",
        "    # sample a batch of test data and partition it into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    batch = data_generator.sample_batch(\"meta_test\", meta_batch_size, shuffle=False, swap=False)\n",
        "    #batch[0].shape(images) = (meta_batch_size, num_classes, k_shot*2, 784)\n",
        "    #batch[1].shape(labels) = (meta_batch_size, num_classes, k_shot*2, num_classes)\n",
        "    input_tr, input_ts = tf.split(batch[0], num_or_size_splits=2, axis=2)\n",
        "    input_tr = tf.reshape(input_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    input_ts = tf.reshape(input_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    label_tr, label_ts = tf.split(batch[1], num_or_size_splits=2, axis=2)\n",
        "    label_tr = tf.reshape(label_tr, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    label_ts = tf.reshape(label_ts, [meta_batch_size, num_classes*k_shot, 784])\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    # meta_test_accuracies.append(result[-1][-1])\n",
        "    meta_test_losses.append(result[3][-1])\n",
        "\n",
        "  # meta_test_accuracies = np.array(meta_test_accuracies)\n",
        "  # means = np.mean(meta_test_accuracies)\n",
        "  # stds = np.std(meta_test_accuracies)\n",
        "  meta_test_losses = np.array(meta_test_losses)\n",
        "  means = np.mean(meta_test_losses)\n",
        "  stds = np.std(meta_test_losses)\n",
        "  ci95 = 1.96*stds/np.sqrt(NUM_META_TEST_POINTS)\n",
        "\n",
        "  # print('Mean meta-test accuracy/loss, stddev, and confidence intervals')\n",
        "  print((means, stds, ci95))\n",
        "\n",
        "\n",
        "def run_maml(n_way=5, k_shot=1, meta_batch_size=25, meta_lr=0.001,\n",
        "             inner_update_lr=0.4, dim_hidden=32, num_inner_updates=1,\n",
        "             learn_inner_update_lr=False,\n",
        "             resume=False, resume_itr=0, log=True, logdir='/tmp/data',\n",
        "             data_path='./omniglot_resized',meta_train=True,\n",
        "             meta_train_iterations=15000, meta_train_k_shot=-1,\n",
        "             meta_train_inner_update_lr=-1):\n",
        "\n",
        "\n",
        "  # call data_generator and get data with k_shot*2 samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
        "\n",
        "  # set up MAML model\n",
        "  dim_output = data_generator.dim_output\n",
        "  dim_input = data_generator.dim_input\n",
        "  model = MAML(dim_input,\n",
        "              dim_output,\n",
        "              num_inner_updates=num_inner_updates,\n",
        "              inner_update_lr=inner_update_lr,\n",
        "              k_shot=k_shot,\n",
        "              dim_hidden=dim_hidden,\n",
        "              learn_inner_update_lr=learn_inner_update_lr)\n",
        "\n",
        "  if meta_train_k_shot == -1:\n",
        "    meta_train_k_shot = k_shot\n",
        "  if meta_train_inner_update_lr == -1:\n",
        "    meta_train_inner_update_lr = inner_update_lr\n",
        "\n",
        "  exp_string = 'cls_'+str(n_way)+'.mbs_'+str(meta_batch_size) + '.k_shot_' + str(meta_train_k_shot) + '.inner_numstep_' + str(num_inner_updates) + '.inner_updatelr_' + str(meta_train_inner_update_lr) + '.learn_inner_update_lr_' + str(learn_inner_update_lr)\n",
        "\n",
        "\n",
        "  if meta_train:\n",
        "    meta_train_fn(model, exp_string, data_generator,\n",
        "                  n_way, meta_train_iterations, meta_batch_size, log, logdir,\n",
        "                  k_shot, num_inner_updates, meta_lr)\n",
        "  else:\n",
        "    meta_batch_size = 1\n",
        "\n",
        "    model_file = tf.train.latest_checkpoint(logdir + '/' + exp_string)\n",
        "    print(\"Restoring model weights from \", model_file)\n",
        "    model.load_weights(model_file)\n",
        "\n",
        "    meta_test_fn(model, data_generator, n_way, meta_batch_size, k_shot, num_inner_updates)\n",
        "  "
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USOh7VulTMK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "2d606dc6-49d8-4b6d-d1e7-6894e32ec084"
      },
      "source": [
        "run_maml(n_way=5, k_shot=1, inner_update_lr=0.4, num_inner_updates=1, dim_hidden=300)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10: pre-inner-loop train loss: 0.16935, post-inner-loop test loss: 0.16920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-202-0e204a7042ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_maml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_update_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inner_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-201-371df1d14970>\u001b[0m in \u001b[0;36mrun_maml\u001b[0;34m(n_way, k_shot, meta_batch_size, meta_lr, inner_update_lr, dim_hidden, num_inner_updates, learn_inner_update_lr, resume, resume_itr, log, logdir, data_path, meta_train, meta_train_iterations, meta_train_k_shot, meta_train_inner_update_lr)\u001b[0m\n\u001b[1;32m    209\u001b[0m     meta_train_fn(model, exp_string, data_generator,\n\u001b[1;32m    210\u001b[0m                   \u001b[0mn_way\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_train_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                   k_shot, num_inner_updates, meta_lr)\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mmeta_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-371df1d14970>\u001b[0m in \u001b[0;36mmeta_train_fn\u001b[0;34m(model, exp_string, data_generator, n_way, meta_train_iterations, meta_batch_size, log, logdir, k_shot, num_inner_updates, meta_lr)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouter_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inner_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_inner_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSUMMARY_INTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-371df1d14970>\u001b[0m in \u001b[0;36mouter_train_step\u001b[0;34m(inp, model, optim, meta_batch_size, num_inner_updates)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_losses_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_ts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss_ts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses_ts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouter_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_losses_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1699\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5527\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   5528\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5529\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5530\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}